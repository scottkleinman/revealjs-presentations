{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A few examples of Python NLTK for Natural Language Processing\n",
    "# Source: https://github.com/DistrictDataLabs/intro-to-nltk/blob/master/NLTK.ipynb\n",
    "\"\"\"\n",
    "import nltk\n",
    "\n",
    "# This is followed by downloading the NLTK corpora, which are already on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the text of _Moby Dick_. Display 10 occurrences of the word \"monstrous\"\n",
    "# with 55 characters on eiher side.\n",
    "moby = nltk.text.Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.concordance(\"monstrous\", 55, lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find words with distributions similar to \"monstrous\"\n",
    "print moby.similar(\"monstrous\")\n",
    "\n",
    "print\n",
    "\n",
    "# Then grab _Sense and Sensibility_ and do the same\n",
    "austen = nltk.text.Text(nltk.corpus.gutenberg.words('austen-sense.txt'))\n",
    "print austen.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Reuters news corpus and get some statistical information\n",
    "reuters = nltk.corpus.reuters\n",
    "counts  = nltk.FreqDist(reuters.words()) # Get word counts\n",
    "vocab   = len(counts.keys()) # Get total number of word types\n",
    "words   = sum(counts.values()) # Get the total number of tokens\n",
    "lexdiv  = float(words) / float(vocab) # Lexical diversity is number of tokens / number of types\n",
    "\n",
    "print \"Corpus has %i types and %i tokens for a lexical diversity of %0.3f\" % (vocab, words, lexdiv)\n",
    "print\n",
    "print \"Most Frequent Type: \" + str(counts.max())\n",
    "print\n",
    "most_common = []\n",
    "for type in counts.most_common(40):\n",
    "    most_common.append(type[0]+\" (\"+str(type[1])+\")\")\n",
    "most_common = \", \".join(most_common)\n",
    "print \"40 Most Common Types:\"\n",
    "print most_common\n",
    "print\n",
    "hapaxes = \", \".join(counts.hapaxes()[0:10])\n",
    "print \"Top 10 Hapax Legomena: \" + hapaxes\n",
    "print\n",
    "percent = format(counts.freq('stipulate') * 100, '.12f')\n",
    "print '% of Corpus Occupied by \"stipulate\": ' + percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the frequencies of the top 200 types\n",
    "counts.plot(200, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse a document into sentences\n",
    "text = u\"Medical personnel returning to New York and New Jersey from the Ebola-riddled countries in West Africa will be automatically quarantined if they had direct contact with an infected person, officials announced Friday. New York Gov. Andrew Cuomo (D) and New Jersey Gov. Chris Christie (R) announced the decision at a joint news conference Friday at 7 World Trade Center. “We have to do more,” Cuomo said. “It’s too serious of a situation to leave it to the honor system of compliance.” They said that public-health officials at John F. Kennedy and Newark Liberty international airports, where enhanced screening for Ebola is taking place, would make the determination on who would be quarantined. Anyone who had direct contact with an Ebola patient in Liberia, Sierra Leone or Guinea will be quarantined. In addition, anyone who traveled there but had no such contact would be actively monitored and possibly quarantined, authorities said. This news came a day after a doctor who had treated Ebola patients in Guinea was diagnosed in Manhattan, becoming the fourth person diagnosed with the virus in the United States and the first outside of Dallas. And the decision came not long after a health-care worker who had treated Ebola patients arrived at Newark, one of five airports where people traveling from West Africa to the United States are encountering the stricter screening rules.\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(text): \n",
    "    print sent\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part-of-Speech (POS) Tagging\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    l = list(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    for token in l:\n",
    "        print(token[0].encode('utf-8') + \": \" + token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demonstrate Zipf's Law on the Brown Corpus.\n",
    "#Source: https://finnaarupnielsen.wordpress.com/2013/10/22/zipf-plot-for-word-counts-in-brown-corpus/\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from itertools import *\n",
    "from pylab import *\n",
    "from nltk.corpus import brown\n",
    "from string import lower\n",
    "from collections import Counter\n",
    "\n",
    "# The data: token counts from the Brown corpus\n",
    "tokens_with_count = Counter(imap(lower, brown.words()))\n",
    "counts = array(tokens_with_count.values())\n",
    "tokens = tokens_with_count.keys()\n",
    "\n",
    "# A Zipf plot\n",
    "ranks = arange(1, len(counts)+1)\n",
    "indices = argsort(-counts)\n",
    "frequencies = counts[indices]\n",
    "loglog(ranks, frequencies, marker=\".\")\n",
    "title(\"Zipf plot for Brown corpus tokens\")\n",
    "xlabel(\"Frequency rank of token\")\n",
    "ylabel(\"Absolute frequency of token\")\n",
    "grid(True)\n",
    "for n in list(logspace(-0.5, log10(len(counts)), 20).astype(int)):\n",
    "    dummy = text(ranks[n], frequencies[n], \" \" + tokens[indices[n]], \n",
    "                 verticalalignment=\"bottom\",\n",
    "                 horizontalalignment=\"left\")\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
